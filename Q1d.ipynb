{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eefedbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as mth\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "444100ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differentiating w.r.t to x\n",
    "def dx(value):\n",
    "    return 6*value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2563f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differentiating w.r.t to \n",
    "def dy(value):\n",
    "    return 4*(value**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f6838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing in terms of f(x,y)\n",
    "def f(x,y):\n",
    "    return 3 * (x**2) + y**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8d71f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now grad of f is\n",
    "def gradf(x, y):\n",
    "    return np.array([6*x, 4*y**3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04a3985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now writing seperate of grad of x and y \n",
    "def gradx(x):\n",
    "    return  6*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8eca8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grady(y):\n",
    "    return 4*y**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ff5187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now hess of f is\n",
    "def hessf(x, y):\n",
    "    return np.array([[6, 0], [0, 12*y**2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fd7142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x,y,alpha,beta):\n",
    "        gradient = gradf(x, y)\n",
    "        no_of_iterations =0\n",
    "        while np.linalg.norm(gradient) >  beta:\n",
    "                x -= alpha * gradx(x)\n",
    "                y -= alpha * grady(y)\n",
    "                gradient = gradf(x, y)\n",
    "                no_of_iterations+=1\n",
    "        return x,y,no_of_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5a01ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtonDescent(x,y,beta):\n",
    "    gradient = gradf(x, y)\n",
    "    hessian = hessf(x,y)\n",
    "    no_of_iterations = 0\n",
    "    while np.linalg.norm(gradient) > beta:\n",
    "        x -= np.linalg.inv(hessian).dot(gradient)[0]\n",
    "        y -= np.linalg.inv(hessian).dot(gradient)[1]\n",
    "        gradient = gradf(x, y)\n",
    "        hessian =  hessf(x, y)\n",
    "        no_of_iterations+=1\n",
    "    return x,y,no_of_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d73edc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Gradient Descent Parameters Are Given Below\n",
      "\n",
      "new X and y are 0.025599999999999984 0.41815746814200305\n",
      "f(x,y) value is 0.5551415091774957\n",
      "Iteration are 4\n",
      "\n",
      "        Newton Parameters Are Given Below\n",
      "\n",
      "new X and y are 0.0 -0.3950617283950617\n",
      "f(x,y) value is 0.024359021445559112\n",
      "Iteration are 4\n",
      "\n",
      "Now, no of iteration to converge is (i.e sum of Gradient Iterations and Newton Iterations)\n",
      "\n",
      "Sum is 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#given value of x and y is 1 and -2\n",
    "# also we are given values for alpha and beta as 0.1 and 0.5\n",
    "x,y= 1,-2\n",
    "alpha,beta = 0.1,0.5\n",
    "# Now Gradient New values of x annd y are\n",
    "print(\"\"\"\n",
    "        Gradient Descent Parameters Are Given Below\n",
    "\"\"\")\n",
    "new_gradient_x,new_gradient_y,gradient_iterations = gradientDescent(x,y,alpha,beta)\n",
    "print(\"new X and y are {} {}\".format(new_gradient_x,new_gradient_y))\n",
    "print(\"f(x,y) value is {}\".format(f(new_gradient_y,new_gradient_y)))\n",
    "print(\"Iteration are {}\".format(gradient_iterations))\n",
    "\n",
    "#Now Newton Descent  New values of x annd y are\n",
    "print(\"\"\"\n",
    "        Newton Parameters Are Given Below\n",
    "\"\"\")\n",
    "new_newton_x,new_newton_y,newton_iterations =newtonDescent(x,y,beta)\n",
    "print(\"new X and y are {} {}\".format(new_newton_x,new_newton_y))\n",
    "print(\"f(x,y) value is {}\".format(f(new_newton_x,new_newton_y)))\n",
    "print(\"Iteration are {}\".format(newton_iterations))\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "Now, no of iteration to converge is (i.e sum of Gradient Iterations and Newton Iterations)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Sum is {}\".format(gradient_iterations+newton_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f955040d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411a8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
